# ==========================================
# TELEGRAM BOT CONFIGURATION
# ==========================================
TELEGRAM_BOT_TOKEN=your_telegram_bot_token_here

# ==========================================
# AI MODE SELECTION
# ==========================================
# Options: 'local' (run LLM locally) or 'openrouter' (use API)
# Local mode requires model download (~4-7GB)
# Default: local
AI_MODE=local

# ==========================================
# OPENROUTER API (if AI_MODE=openrouter)
# ==========================================
OPENROUTER_API_KEY=your_openrouter_api_key_here
OPENROUTER_API_URL=https://openrouter.ai/api/v1/chat/completions
AI_MODEL=meta-llama/llama-3.1-8b-instruct

# ==========================================
# LOCAL LLM CONFIGURATION (if AI_MODE=local)
# ==========================================
# Path to pre-downloaded model (optional, will auto-download if not specified)
LOCAL_MODEL_PATH=

# CPU threads for inference (adjust based on your CPU)
# Default: 16
LOCAL_MODEL_THREADS=16

# Context window size (max tokens)
# Default: 4096
LOCAL_MODEL_CONTEXT=4096

# Temperature for generation (0.0-1.0, higher = more creative)
# Default: 0.7
LOCAL_MODEL_TEMPERATURE=0.7

# ==========================================
# RAG (Retrieval-Augmented Generation)
# ==========================================
# Enable RAG system for document-based answers
# Default: true
RAG_ENABLED=true

# Directory to store RAG database
# Default: ./rag_data
RAG_PERSIST_DIR=./rag_data

# Collection name in vector database
# Default: financial_docs
RAG_COLLECTION_NAME=financial_docs

# Number of documents to retrieve for context
# Default: 3
RAG_TOP_K=3

# Maximum tokens for RAG context
# Default: 2000
RAG_MAX_CONTEXT=2000

# ==========================================
# TRANSLATION SETTINGS
# ==========================================
# Enable translation for models trained on English
# Схема: RU → EN → LLM → RU
# Улучшает качество ответов для англоязычных моделей
# Default: false
TRANSLATION_ENABLED=false

# Device for translation models
# Options: cpu, cuda
# Note: OPUS-MT models work fine on CPU (~0.5s per 100 words)
# Default: cpu
TRANSLATION_DEVICE=cpu

# ==========================================
# DATABASE CONFIGURATION
# ==========================================
DB_HOST=localhost
DB_PORT=5432
DB_NAME=your_database_name
DB_USER=your_database_user
DB_PASSWORD=your_database_password

